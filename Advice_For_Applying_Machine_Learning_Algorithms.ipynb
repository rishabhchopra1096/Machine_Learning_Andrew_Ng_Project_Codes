{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging a learning algorithm: \n",
    "Suppose you haver implemented regularized linear regresion to predict housing prices with the following cost function. \n",
    "\n",
    "## $ J(\\theta) = \\frac{1}{2m}[ \\sum_{i=1}^m (h{_\\theta}x^{(i)} - y^{(i)} + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$  \n",
    "\n",
    "However, your hypothesis on a new set of houses, you find that it makes unacceptably large errors in its predictions. \n",
    "\n",
    "__What do you do next?__ ( See Challenges of Machine Learning, C1) \n",
    "- Get more training examples (does not always help) \n",
    "- Try smaller set of features \n",
    "- Try getting additional features \n",
    "- Try adding polynomial features\n",
    "- Try decreasing $\\lambda$\n",
    "- Try increasing $\\lambda$\n",
    "\n",
    "### But what should we select? \n",
    "\n",
    "__Machine Learning diagnostic__: \n",
    "- A test that you can run to gain insgiht what is / isn't working with a learning algorithm, and gain guidance as to how best to improve its performance.\n",
    "- Diagnostics can take time to implement, but doing so can be very good use of yout time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Evaluate a hypothesis/model that had been learnt by your algorithm :\n",
    "\n",
    "- Overfitting hypothesis fails to generalize to new examples that are not in the training set. \n",
    "- Now, in univariate linear regression, it is easy to plot and see if the model is overfitting to the training data. \n",
    "- But when we have multivariate linear regression, it becomes almost impossible to plot what the hypothesis function looks like. \n",
    "- __Train/Test prodecure for linear regression__\n",
    "    - Split the data into Training and Testing set\n",
    "    - Learn parameters $\\theta$ vector from the training data (while minimising $J(\\theta)$. \n",
    "    - Compute the test set error:\n",
    "        - ## $ J_{test}(\\theta) = \\frac{1}{2m_{test}}[ \\sum_{i=1}^{m_{test}} (h{_\\theta}x_{test}^{(i)} - y_{test}^{(i)}]^2$  \n",
    "- __Train/Test procudure for logistic regression__\n",
    "    - Split the data into Training and Testing set\n",
    "    - Learn parameters $\\theta$ vector from the training data (while minimising $J(\\theta)$. \n",
    "        - Compute the test set error: \n",
    "        - #### $ J(\\theta) = \\frac{1}{m_{test}}[ \\sum_{i=1}^{m_{test}} y_{test}^{(i)}log(h{_\\theta}x_{test}^{(i)}) + (1 - y_{test}^{(i)})log(h{_\\theta}x_{test}^{(i)}) $\n",
    "        - Misclassification error (0/1 misclassifcation error)\n",
    "\n",
    "    $ Err(h_{\\theta}x, y) = [$\n",
    "\n",
    "    $1 $ if $ h_{\\theta}x >= 0.5$ and $y = 0,$ \n",
    "\n",
    "    $1 $ if $ h_{\\theta}x <= 0.5$ and $y = 1, $\n",
    "\n",
    "    $0$,  otherwise \n",
    "\n",
    "    $]$\n",
    "- The above will give us a binary 0 or 1 error result baed on misclassification. \n",
    "- The average test error for the test set is: \n",
    "- ### $Test Error=\\frac{1}{m_{test}}\\sum_{i}^{m_{test}}=Err(hÎ˜(x(i)test),y(i)test)$\n",
    "- This gives us the proportion of the test data that was misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and training/validation/test:  \n",
    "\n",
    "Once the parameters $ \\theta_0, \\theta_1, ..., \\theta_3..$ were fit to some training set, the error of the parameters as measured on the training set is likely to be lower than the actual generalization error. \n",
    "\n",
    "- Lets say you're trying to choose which degree of polynomial do you want to pick? $ d= 1 $ , $d = 2$, ... $d = n$ \n",
    "- How to find a measure of well each degree model generalises? \n",
    "    - Minimise the cost function and find the vector of parameters for each degree model. \n",
    "    -  $ \\theta^1, \\theta^2, ..., \\theta^n..$, where n is the degree of the model. \n",
    "    - Then, for each parameter vectors, calculate the performance on the test set. \n",
    "    - ## $ J_{test}(\\theta) = \\frac{1}{2m_{test}}[ \\sum_{i=1}^{m_{test}} (h{_\\theta}x_{test}^{(i)} - y_{test}^{(i)}]^2$  \n",
    "    - $ J_{test}(\\theta^1), J_{test}(\\theta^2), ... J_{test}(\\theta^n) $ \n",
    "    - In order to select one of the models, select the model with the least testing error. \n",
    "    - But the __problem here is that we used the test set for choosing the degree of the polynomial__. We should never use the test set to figure out the parameters/hyperparameters of the model. Because of this, we will get an overly optimistic estimate of the generalization error, when we do check it. \n",
    "    -  Instead, we can just split the data further: \n",
    "        - Training Test: For training the models and finding the parameters. \n",
    "            - ### $ J_{train}(\\theta) = \\frac{1}{2m_{train}}[ \\sum_{i=1}^{m_{train}} (h{_\\theta}x_{train}^{(i)} - y_{train}^{(i)}]^2$  \n",
    "        - Cross Validation Test: For fining the right hyperparameters for the learning algorithm. \n",
    "            - ### $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}[ \\sum_{i=1}^{m_{cv}} (h{_\\theta}x_{cv}^{(i)} - y_{cv}^{(i)}]^2$  \n",
    "        - Test Set : For confirming that your model generalises well.\n",
    "            - ### $ J_{test}(\\theta) = \\frac{1}{2m_{test}}[ \\sum_{i=1}^{m_{test}} (h{_\\theta}x_{test}^{(i)} - y_{test}^{(i)}]^2$  \n",
    "    - Typical split looks like 60% Training, 20% Cross Validation and 20% Testing Set. \n",
    "    - __Now, how to select the final model ? __\n",
    "        - Minimise the cost function and find the vector of parameters for each degree model. \n",
    "        -  $ \\theta^1, \\theta^2, ..., \\theta^n..$, where n is the degree of the model. \n",
    "        - Then, for each parameter vectors, calculate the performance on the test set. \n",
    "            - ### $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}[ \\sum_{i=1}^{m_{cv}} (h{_\\theta}x_{cv}^{(i)} - y_{cv}^{(i)}]^2$  \n",
    "        - In order to select one of the models, select the model with the least validation error.\n",
    "        - Now that we have the test set, untouched. We can evaluate the generalisation error of the model with out test set. \n",
    "            - ### $ J_{test}(\\theta) = \\frac{1}{2m_{test}}[ \\sum_{i=1}^{m_{test}} (h{_\\theta}x_{test}^{(i)} - y_{test}^{(i)}]^2$  \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Diagnosing Bias vs. Variance: \n",
    "Suppose your learning algorithm is performing less well than you were hoping. That is, the error of the cross validation set or test set is high. \n",
    "- __Is it a bias problem or varance problem__?\n",
    "- This is crucial to understand in order find the way to fix the model. \n",
    "- Now, plot a graph: \n",
    "    - __X AXIS__ : Degree of Polynomial(Model Complexity):\n",
    "    - __Y AXIS__: \n",
    "        - ### $ J_{train}(\\theta) = \\frac{1}{2m_{train}}[ \\sum_{i=1}^{m_{train}} (h{_\\theta}x_{train}^{(i)} - y_{train}^{(i)}]^2$    \n",
    "        \n",
    "        and\n",
    "        \n",
    "        - ### $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}[ \\sum_{i=1}^{m_{cv}} (h{_\\theta}x_{cv}^{(i)} - y_{cv}^{(i)}]^2$     \n",
    "\n",
    "\n",
    "\n",
    "<img src = \"images/Advice_For_Applying_ML_algos/Bias_vs_Variance.png\">\n",
    "\n",
    "- High bias referes to underfitting. High variance refers to overfitting. Ideally, we need the golden mean between the 2. \n",
    "- The training error will always tend to decrease as we increase the complexity of the model. \n",
    "- The cross validation error will tesnd to decrease as we increase the complexity of the model, but only uptil a certain point. From there, the cross validation error will start to increase as d is increased, forming a convex curve. \n",
    "- High Bias (Underfitting): The error on the training set, as well as on the cross validation set is large.\n",
    "- High Variacne (Overfitting): The error on the training set is too low, and the error on the validation set is large. \n",
    "\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and Bias/Variance:   \n",
    "\n",
    "<img src = \"images/Advice_For_Applying_ML_algos/regularization_bias_variance.png\">  \n",
    "\n",
    "In the figure above, we see that as $\\lambda$ increases, our fit becomes more rigid. ( High Bias ) \n",
    "On the other hand, as $\\lambda$ decreases, we tend to overfit the data. ( High Variance )\n",
    "- __How do we choose the best regularization term lambda__? \n",
    "    - Create a list of lamdas, starting from $ 0, 0.1 $ and then multiply by $2$ , $ 0.02, 0.04, 0.08, 0.16 $ .. and so on. \n",
    "    - Create a set of models with different degrees or any other variants (model complexity).\n",
    "    - Iterate though the lamdas and for each lamda, go though al the models to learn some parameter vector $\\theta$ using the following cost function: \n",
    "        - ### $ J(\\theta) = \\frac{1}{2m}[ \\sum_{i=1}^m (h{_\\theta}x^{(i)} - y^{(i)} + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$ \n",
    "    - So, if you have 5 different degree models. And you have a list of 10 lamdas. You will train 50 models. Each having a different model complexity - regularization term combination. \n",
    "    - Compute the cross validation error using the learned parameter vectors. That is, for example calculate 50 cross validation errors. \n",
    "         - ### $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}[ \\sum_{i=1}^{m_{cv}} (h{_\\theta}x_{cv}^{(i)} - y_{cv}^{(i)}]^2$     \n",
    "    - Choose the model(parameter vectors) with a model complexity - regularization term combination which gives the least cross validation error. \n",
    "    - Once you have the parameter vectors __$\\theta$__ using the best model complexity-regularization term combination, calculate the generalisation error on the test set, i.e $ J_{test}(\\theta)$. \n",
    "        - ### $ J_{test}(\\theta) = \\frac{1}{2m_{test}}[ \\sum_{i=1}^{m_{test}} (h{_\\theta}x_{test}^{(i)} - y_{test}^{(i)}]^2$  \n",
    "\n",
    "- If you plot the training and testing errors as a function of the regularization term $\\lambda$. \n",
    "- You would get a graph that looks like this:\n",
    "<img src = \"images/Advice_For_Applying_ML_algos/regularization_train_val_error.png\">  \n",
    "- Note that the training and validation errors are calulated without the regularisation term. \n",
    "- ### $ J(\\theta) = \\frac{1}{2m}[ \\sum_{i=1}^m (h{_\\theta}x^{(i)} - y^{(i)} + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$ is only used to find the parameter vectors of the model using different lambdas. \n",
    "- Training Error:  \n",
    "    - ### $ J_{train}(\\theta) = \\frac{1}{2m_{train}}[ \\sum_{i=1}^{m_{train}} (h{_\\theta}x_{train}^{(i)} - y_{train}^{(i)}]^2$  \n",
    "- Validation Error: \n",
    "    - ### $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}[ \\sum_{i=1}^{m_{cv}} (h{_\\theta}x_{cv}^{(i)} - y_{cv}^{(i)}]^2$     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves: \n",
    "- Plots the Average Training Error: \n",
    "    - ### $ J_{train}(\\theta) = \\frac{1}{2m_{train}}[ \\sum_{i=1}^{m_{train}} (h{_\\theta}x_{train}^{(i)} - y_{train}^{(i)}]^2$  \n",
    "- and Average Validation Error:\n",
    "    - ### $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}[ \\sum_{i=1}^{m_{cv}} (h{_\\theta}x_{cv}^{(i)} - y_{cv}^{(i)}]^2$  \n",
    "- as function of the training set size starting from $ m =1 $ to $ m = m $.  \n",
    "- Training an algorithm on a very few number of data points (such as 1, 2, or 3) will easily hvae 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence: \n",
    "    - As the training set gets larger, the error for the training set decreases.\n",
    "    - The error will plateau out after a certain m, or trainning set size. \n",
    "\n",
    "- __Training Error__ : When m is small, you can easily fit the training set, but as m grows, it becomes difficult to fit a model with a fixed model complexity that goes through all of the points in the training set. \n",
    " - __Validation Error__ : Whem m is small, we would not be able to generalise well, but as m grows, the cross validation error will decrease.\n",
    "  \n",
    "- __When Experiencing high bias (underfitting): __\n",
    "    - __Low Training Set Size:__ cause the error on training set to be low , and the error on the validation set to be high. \n",
    "    - __High Training Set Size:__ \n",
    "        - _Training Error_ : As the the number of training examples increase, a low compexity model can only do so much. The training error will increase and plateau at a high point. \n",
    "        - _Validation Error_: As the number of training examples increase, a low complexity model will learn more from the training set and will be able to generalise a bit better as m increases. But again, it will plateau at a relatively lower point than the intial cross validation error (but still high). \n",
    "    - The following image proves that if a learning algorithm is experienceing high bias (underfitting), just increasing the number of training examples will not help, as a high bias model will just keep over-simplifying the problem ( fitting a line to a high variance dataset) . Its cross validation error will plateau at a relatively lower (but still high) point. \n",
    "    \n",
    "__Learning Curves of a fixed high bias model__:     \n",
    "<img src =\"images/Advice_For_Applying_ML_algos/learning_curves_of_high_bias_model.png\" > \n",
    "\n",
    "- __When experiencing high variance:__\n",
    "    - __Low Training Set Size:__ cause the error on training set to be low (overfitting to small number of examples) , and the error on the validation set to be high (still not able to generalise to random examples) . \n",
    "    - __High Training Set Size:__\n",
    "        - _Training Error_: With a highly complex model, the error on the training set will start of as none, or very low. Once the training set size increases, it will get difficult to catch all the points in the training set. But the training error will still mainitain a low. \n",
    "        - _Validation Error_: With a higly complex model, the error on the validation set will start of as high, but will eventually start decreasing. Not too much though, as it is overfitting the training set. This will create the gap between the training error(low) and validation error(high). \n",
    "            - But, then if we give a highly complex model more data to learn from, it will be forced to generalise. \n",
    "            - Think of it like a person who has 10 degrees of learning capacity. But if we start giving the person 50 points, the person will be forced to generalise. \n",
    "            - Similarly, the weights of the highly complex model will be oriented towards generalising if we give it a huge training set. \n",
    "            \n",
    "    - The following image proves that if a learning algorithm is experienceing high variance (overfitting), increasing the number of training examples will probably make the validation error and the training error converge to a low point.   \n",
    "    \n",
    "__Learning Curves of a fixed high variance model__:     \n",
    "<img src =\"images/Advice_For_Applying_ML_algos/learning_curves_of_high_variance_model.png\" > \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding what to do next (Revisited) : \n",
    "\n",
    "Suppose you haver implemented regularized linear regresion to predict housing prices with the following cost function. \n",
    "\n",
    "## $ J(\\theta) = \\frac{1}{2m}[ \\sum_{i=1}^m (h{_\\theta}x^{(i)} - y^{(i)} + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$  \n",
    "\n",
    "However, your hypothesis on a new set of houses, you find that it makes unacceptably large errors in its predictions. \n",
    "\n",
    "__What do you do next?__ ( See Challenges of Machine Learning, C1) \n",
    "\n",
    "- __Useful for a high variance problem__\n",
    "    - Get more training examples (does not always help)\n",
    "    - Try smaller set of features \n",
    "    - Try increasing $\\lambda$\n",
    "\n",
    "- __Useful for high bias problem__\n",
    "    - Try getting additional features \n",
    "    - Try adding polynomial features\n",
    "    - Try decreasing $\\lambda$\n",
    "    \n",
    "### An example: Diagnosing Neural Networks: \n",
    "- A neural network with fewer parameters is __prone to underfitting__ ( will perform porrly on the training as well as the validation set), but the upside is that it's __computationally cheaper__. \n",
    "- A neural network with more parameters than needed is __prone to underfitting__ ( will perform extremely well on the training set but fail to generalise ). The downside of this is that it's __computationally expensive__. \n",
    "    - Using Regularisation with a high vairance model is preferred over using just a high bias model. \n",
    "- Using a single hidden layer is a good starting default. \n",
    "- As an example:\n",
    "    - You can plot the model complexity graph. \n",
    "    - You can train neural netowrks with different number of hidden layers/nodes on the training set, and compare there performance using the validation set. \n",
    "    - Finally, once you pick the best #layer , #nodes comibination (lowest cv error) , you can check how well it generalises on the testing set. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
