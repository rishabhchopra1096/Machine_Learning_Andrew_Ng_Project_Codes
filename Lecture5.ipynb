{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cancer: Malignant Vs Benign:__\n",
    "- What does logitic regression do? \n",
    "    - Given $(x,y)$ pairs where $x$ is the size of the tumor and $y \\in (1,0)$ where $1$ means malignant and $0$ means bengn. \n",
    "    - We plot the data: $(x,y)$\n",
    "    - Then, we intialise random weights. \n",
    "    - Then, we predict $\\theta.T.X$ linear combination: \n",
    "        - $ \\theta_0 + \\theta_1x_1 $\n",
    "        - Run it through the sigmoid function $ \\frac{1}{1 + e^{-x}}$ \n",
    "        - If the resulting activation is beyond a threshold, we predict $1$, else we predict $0$. \n",
    "    - Then, we compute the error, and perform gradient descent to update the weights iteratively. The gradient descent formula is: \n",
    "        - ## $\\theta_j := \\theta_j - \\eta \\frac{\\partial {J(\\theta)}}{\\theta_j}$\n",
    "        - We'll take the partial derivative of the cost function w.r.t each weight. \n",
    "        - ## $\\theta_j := \\theta_j - \\eta*\\sum_{i=1}^m(\\sigma(x_i) - y_i)x_j^i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's something else we can do: \n",
    "    - We can go through our training set and look through al the positive examples. See what malignant tumors look like. \n",
    "    - And then we can build a model of how malignant tumors look like. \n",
    "    - Then, we can go through all the benign tumors and build a model of how benign tumours look like. \n",
    "    - Then, when get a new tumor size, you can match it both the models. We can predict on whether a tumor is malignant or benign depending on which model the new training example matches more to. \n",
    "- This is an example of Generative Learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The models we have learnt uptil now are all discriminative learning algorithms.\n",
    "    - These are the model that learn $P(y|x)$(_The probability of the y label being positive, given the features_) directly, or learn $h_{\\theta}x \\in (0,1)$ directly. \n",
    "- In contrast:\n",
    "    - A generative learning algorithm model the $P(x|y)$ ( The probability of the feature belonging to a certain class, y). \n",
    "    - It also models ($P(y)$) (_probabiltity of picking a datapoint with label y_) \n",
    "    - By modelling $P(x|y)$, you can use bayes rule to find out the ($P(y|x)$). \n",
    "    - # $P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$\n",
    "    - Then, you can also find the probabiltity of the features: \n",
    "        - ### $P(x) = P(x|y = 1)p(y = 1) + P(x|y =\n",
    "0)p(y = 0)$ DOUBT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example of Generative Learning Algorithm__:  \n",
    "- Assume $x \\in \\mathbf{R}^n$, and continuous valued. \n",
    "\n",
    "### Gaussian Distribution Analysis: \n",
    "    - https://www.youtube.com/watch?v=iYiOVISWXS4\n",
    "- P(x|y) is "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
