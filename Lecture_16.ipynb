{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "- While teaching a helicopter how to fly, your job is to input the orientation and position of the the helicopter and output a set of numbers that correspond to where to move the control stick, to execute whatever you want.  \n",
    "- This is different from supervised learning. \n",
    "- We do not know what exactly the \"right answer\" is. We cannot come up with a traning set of X (the position and orientation, 2 features) and the output y (the right actions to take). There can be multiple right answers here, and it's really tough to come up with a comprehensive training set. \n",
    "- Using a training set won't even work. This is something different.  \n",
    "- We'll come up with a reward signmal which will measure how well the agent/helicopter is doing and it will be the job of the learning algorithm to take reward functions as inputs and try to fly well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another example would be to teach an agent to play chess. \n",
    "- We cannot come up with a training set with X's being the board positions and the y's being the optimal move to make. This is because we cannot have one optimal move at a position! There are multiple scenarios! The optimal move depends on the scenario and there might be multiple optimal moves.  \n",
    "- We can give the agent a positive reward when it wins a game, and negative reward when it loses a game. And hopefully, it'll understand how to play the game eventually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Why is Reinforcement Learning harder than Supervised Learning? __ \n",
    "- This is not a one shot decision making process. \n",
    "- In Supervised Learning, you make a prediciton and then you're done. The person either had cancer or does not. That's the end of the process.\n",
    "- In Reinforcement Learning, you have to make actions over time (sequential decision making).   \n",
    "- Suppose your agent plays a game of chess and losses on the 60th move. Now, before the 60th move it did not get any feedback (reward). On the 60th move it got a negative reward. This makes it hard for the agent to learn as you do not know where you went wrong. This is known as the _Credit Assignment Problem_.   Suppose you went wrong on the 23rd move, and then fixes that on the 50th move, and then blunders on the 60th move... Which move do you assign blame to? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement Learning problems model the world using the __MDP (Markov Decision Process)__.  \n",
    "- An MDP is a 5 tuple: \n",
    "    - $S$: set of states, eg:set of all possible positions and orientations of the helicopter. \n",
    "    - $A$: set of all actions. eg: all positions we can put our control stick into. \n",
    "    - $P_{SA}$: state transition distributions. For each state and each action, this is a probability distribution. \n",
    "        - $\\sum_{S'}P_{SA}(S') = 1$ The sum of all the probabilities of state you may end up in, $S'$, after taking actin $A$ with current state $S$ is $1$. \n",
    "        - $P_{SA}(S') \\geq 0$ : The probability of ending up in state $S'$ after taking action $A$ in state $S$ is always $geq$ 0. In other words, there is always a possibility take you are safe at one moment, flying $S$ and then one action $A$, leads to a new state $S'$ of crash :p. \n",
    "        -  State Transition Distributions/State Transition Probabilities work as follows: \n",
    "            - $P_{SA}$ gives me the probability distribution over what state will i transition to if i take the action $A$, in the state $S$. \n",
    "    - $\\gamma$ is a number known as a discount factor. It is always between 0 and 1 ($0 \\leq \\gamma < 1$). \n",
    "    - $R$ is our reward function. $R: S -> R$ The reward function maps from a set of states to the real numbers and it can be positive or negatve. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- __Example:__  \n",
    "<img src = \"images/mdp.png\"> \n",
    "    - $S$: 11 states \n",
    "    - $A$: actions, {N, E, W, S}\n",
    "    - $P_{SA}$\n",
    "        - $P_{(3,1),N}(3,2) = 0.8$  \n",
    "        - $P_{(3,1),N}(4,1) = 0.1$\n",
    "        - $P_{(3,1),N}(2,1) = 0.1$ \n",
    "        - $P_{(3,1),N}(3,3) = 0$ \n",
    "    - $R$:\n",
    "        - $R((4,3)) = +1$\n",
    "        - $R((4,2)) = -1$ \n",
    "        - $R((S)) = -0.02$ for all other states. (charges the robot for just wandering around (battery consumption).   \n",
    "- In this specific example, we are assuming that there are no states once we reach $(4,3)$ or $(4,3)$. There are no more rewards after that. \n",
    "- There are many ways to model that. One way is to think of a $12^{th}$ state called the $0$ cost absorbing state. So once you get to the $+1$ or $-1$, you then transition with probability $1$ to this $12^{th}$ state which has no more rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ How does it work? __\n",
    "- You start with being in some state $S_0$.  \n",
    "- You then take some actiona $A_0$. \n",
    "- You get to $S_1$ drawn randomly from $P_{S_0A_0}$ distribution. \n",
    "- Choose another action $A_1$.  \n",
    "- You get to $S_2$ drawn randomly from $P_{S_1A_1}$ distribution.   \n",
    "- So at the end of it you get a sequence of states ($S_0, S_1, S_2, ..... $) \n",
    "- And to evaluate how well we did, we will add up all the rewards the robot obtained at each state: \n",
    "    - $ R(S_0) + \\gamma.R(S_1) + \\gamma^2.R(S_2) + ..... $ \n",
    "- This is known as the _total payoff_ for the sequnce of states your robot visited.  \n",
    "- $\\gamma$'s effect is that it reduces the reward or _\"discounts\"_ the reward as steps increase. The reward you get at state 1 is bigger than reward at state 2, which is bigger than reward at state 3...... \n",
    "    - A dollar today is worth a little more than a dollar tomorrow. \n",
    "    - Conversely, having to pay out a dollar tomorrow is better than having to payout a dollor today.  \n",
    "    - Basically, $\\gamma$ weighs wins and losses in the immediate future more than wins and losses in the distant future.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the reinforcement learning algorithm is to choose actions over time to try to maximise the expected value of the _total payoff_.  \n",
    "* $ R(S_0) + \\gamma.R(S_1) + \\gamma^2.R(S_2) + ..... $   \n",
    "Concretely, what we will try to compute is try to make our reinforcement learning algorithms compute a _policy_ $\\pi: S -> A$. A _policy_ just a function mapping from a state to an action. So a _policy_ basically will tell us, \"given a state, which action should be taken\".    \n",
    "So for the above example, here is the _optimal policy_:  \n",
    "\n",
    "<img src = \"images/policy.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This is the _optimal policy_  in the sense that when you implement this policy, you will get the maximum _total payoff_ (total sum of discounted rewards) possible.    \n",
    "\n",
    "Executing the _policy_ means taking the actions according to the _policy_.    \n",
    "\n",
    "Notice $(3,1)$ goes to the west and not the north, which is the shorter path. This is because if it goes north to $3,2$,  there is a $0.1$ possibility that you end up at $4,2$ which gives a reward of $-1$.   \n",
    "Therefore, it is better tot take ther longer and safer route rather than the shorter and dangerous.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining $V^{\\pi} , V^*$ and $\\pi^*$   \n",
    "- It'll be the consequence of these definintions that $\\pi^*$ will be the optimal policy.    \n",
    "\n",
    "### $V^{\\pi}$:  \n",
    "- For any policy $\\pi$, we are going to define a value function $V^{\\pi}: S -> R$ s.t $V^{\\pi}(S)$ is the expected total payoff starting in state $S$ and excuting policy $\\pi$.     \n",
    "## $V^{\\pi}(S) = E[R(S_0) + \\gamma.R(S_1) + ... | \\pi , S_0 = S)] $  \n",
    "\n",
    "__Concrete Example: __    \n",
    "\n",
    "<img src = \"images/V_pi_example.png\" >   \n",
    "\n",
    "The above example shows a policy on the left and a value function on the right. The value function tells us the _total expected payoff_ when we start from a particular _state_.  \n",
    "So, if we start from $(1,3)$ , we get a positive payoff of $.52$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathbf{V^{\\pi}(S) = E[R(S_0) + \\gamma(R(S_1) + \\gamma(R(S_2)) +\\gamma(R(S_3)) + ... )| \\pi , S_0 = S)]} $    \n",
    "    - ### $R(S_0)$: \n",
    "        - is sometimes known as the immediate reward. It is the reward you get for starting at this state. \n",
    "    - The consequent steps are known as _future rewards_.  \n",
    "    - ### $R(S_1) + \\gamma(R(S_2)) +\\gamma(R(S_3)) + ... )$ is $V^{\\pi}(S_1)$: \n",
    "        - the steps you would go through if you had started with $S_1$.   \n",
    "    - $\\mathbf{V^{\\pi}(S_0) = E[R(S_0) + \\gamma(R(S_1) + \\gamma(R(S_2)) +\\gamma(R(S_3)) + ... )| \\pi , S_0 = S)] }$  \n",
    "    - $\\mathbf{V^{\\pi}(S_0) = R(S_0) + \\gamma[\\sum_{S_1}P_{S_0\\pi(S_0)}(S_1)V^{\\pi}(S_1)]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman's Equations\n",
    "## $V^{\\pi}(S) = R(S) + \\gamma\\sum_{S'}P_{S\\pi(S)}(S')V^{\\pi}(S')$ \n",
    "- Why are the probabilities of transitioning to as tate written as $\\mathbf{P_{S \\pi(S)}(S')}$ and not $\\mathbf{P_{S A}(S')}$ ? \n",
    "    - Because, here we are following a policy. The action we will take in state $S$ will be $\\pi(S)$. \n",
    "- So what this equation is saying is the same thing as the above equations. You start with your first state $S_0$ / $S$, here. Then, further, your steps are basically $\\gamma$ times the value function of the next state you enter in$S'$.  Since $S'$(the next state) is ranomly distributed, you first need to account for its probability, and then multiply it by the value function of $S'$.   \n",
    "\n",
    "- Bellman's Equations gives you a way to solve for the value function for a policy.  \n",
    "- Suppose you are given a fixed policy. How to you solve for $V^{\\pi}?$.  \n",
    "- You can write down Bellman's equation for every state in your MDP and this imposes a set of linear constraints on what the value function could be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Concretely: __  \n",
    "## $V^{\\pi}(S) = R(S) + \\gamma\\sum_{S'}P_{S\\pi(S)}(S')V^{\\pi}(S')$   \n",
    "Let's say $\\pi(3,1) = N$   \n",
    "$V^{\\pi}(3,1) = R(3,1) + \\gamma[0.8$$\\mathbf{V^{\\pi}(3,2)}$ $ + 0.1$$\\mathbf{V^{\\pi}(4,1)}$ + 0.1$\\mathbf{V^{\\pi}(2,1)}] $  \n",
    "The above is the bellman equation for the $(3,1)$ state. It is the immediate reward , $R(3,1)$, plus gamma times the sumation of the all the probable (value functions, states) it could go in. $[0.8$_$\\mathbf{V^{\\pi}(3,2)}$_ $ + 0.1$$\\mathbf{V^{\\pi}(4,1)}$ + 0.1$\\mathbf{V^{\\pi}(2,1)}] $  \n",
    "- Notice, the value functions in __bold__ are the unknowns here. If we write bellman's equation for each of the states, we can solve the linear equations to get a value function for a particular state. \n",
    "- This was the value function for a specific given policy and how to solve for it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Value Function: $ V^*(S) $ \n",
    "### $V^*(S) = max_{\\pi} V^\\pi(S)$ \n",
    "- For any given state, $S$, the optimal value function, suppose i take a $max$ over all possible policies $\\pi$, what is the best possible expected sum of discounted rewards that i can get. \n",
    "- So basically, given a state, what is the best policy i can implement (that will of course, give me the best _total payoff_).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman's Eqautions for $V^*(S)$:  \n",
    "## $V^*(S) = R(S) + \\mathbf{max_A} \\gamma\\sum_{S'}P_{SA}(S')V^*(S')$  \n",
    "- My optimal expected total payoff is my immediate reward plus the best action i can choose of my expected future payoff.    \n",
    "- DOUBT: The max should be for every value function, not just for the second value function: \n",
    "    - Basically what this is saying is that, you are following the optimal policy, which will give you the optimal reward once you reach the next state. \n",
    "    - You just need to make sure that you execute the best action possible such that it maximises the future rewards. \n",
    "    - Concretely, you are in some state $s$, you take some action $a$ and then, depedning on that action, you expected total payoff will be calculated using : $\\mathbf{\\gamma\\sum_{S'}P_{SA}(S')V^*(S')}$. \n",
    "    - So, the only thing you need to be careful about is, taking the right action. \n",
    "    - Once you take the right action, you canbe sure that the optimal policy will take care of the rest. \n",
    "## ${\\pi}^*(S) = \\mathbf{argmax_A}  \\sum_{S'}P_{SA}(S')V^*(S')$   \n",
    "- When you are in some state $s$, you are definitely going to get an immediate reward for taking any action in state $s$. \n",
    "- Now, what will differentiate that reward among different actions is the consequent rewards from states you might end up in. \n",
    "- You want to choose the action that maximises the total expected reward from the current state. \n",
    "- Start by taking one action, $a$ from state $s$. Calculate the $probability*value$ $function$ pairs for all the states that you might end up in after taking action $a$. Sum them. Do this for all the actions possible. Choose the action that reaps the best reward from state $s$. \n",
    "- __Tells you what is the best action to take in each state by taking the argmax.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice, if we can compute $V^*(S)$, _the optimal value function_, we can input it into the equation for $\\pi^*(S)$ to get the _optimal policy_  \n",
    "- So, the strategy for computing the _optimal policy_ would be to compute $V*(S)$, which the maximum return you can get starting from state $s$ and following policy $*$. \n",
    "- But the definition of $V^*(S) = max_{\\pi} V^\\pi(S)$  does not really help us to compute it cause there are an exponentially large number of policies. This is because if you 4 actions and 11 states, then the number of policies is $4^{11}$.   \n",
    "- We know how to compute $V^{\\pi}$: \n",
    "    - For any policy $\\pi$, we are going to define a value function $V^{\\pi}: S -> R$ s.t $V^{\\pi}(S)$ is the expected total payoff starting in state $S$ and excuting policy $\\pi$.   \n",
    "- by solving the system of linear equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration:  \n",
    "- Initialise $V(S) = 0$ $\\forall S$: Create an array of $n$ elements where $n$ is the the number of states. \n",
    "- For every S, update:  \n",
    "    - $V(S) := R(S) + max_A \\gamma \\sum_S' P_{SA}(S')V(S')$  \n",
    "- This will make $V(S) -> V^*(S)$ \n",
    "- DOUBT: Which policy are we following here while computing RHS? Or is this just random?   \n",
    "\n",
    "- Synchronous Update: Symultaneously updateing all $V(S)$. \n",
    "- Asynchronus Update: Update $V(S)$ one by one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- When you compute the value function using the value iteration and $\\gamma = 0.99$.  \n",
    "- The number you get for $V^*$ are as follows: \n",
    "\n",
    "<img src=\"images/v_star.png\">\n",
    "\n",
    "- Again, this is the maximum reward you can get from starting from state $s$ and following the optimal policy. \n",
    "- Now, how do we get the optimal policy once we compute the optimal value function? \n",
    "- We use the following equation: \n",
    "    - ${\\pi}^*(S) = \\mathbf{argmax_A}  \\sum_{S'}P_{SA}(S')V^*(S')$   \n",
    "- which gives us the optimal policy: \n",
    "<img src = \"images/policy.png\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration: \n",
    "- Initialise the policy $\\pi$ randomly. \n",
    "- Repeat: \n",
    "    - Let $V := V^{\\pi}(S)$\n",
    "        - $\\mathbf{V^{\\pi}(S) = R(S) + \\gamma\\sum_{S'}P_{S\\pi(S)}(S')V^{\\pi}(S')}$  \n",
    "    - Let $\\pi(s) := \\mathbf{argmax_A}  \\sum_{S'}P_{SA}(S')V^{\\pi}(S')$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DOUBT: How is it getting better at every loop? \n",
    "    - Every time you go through the loop, you calculate the value function for all the states using current policy. \n",
    "    - Then you calculate a new policy, that takes the best out of what was given. \n",
    "    - So this kinda helps me understand how this is getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if you do not know the State Transition Probability?   \n",
    "- Let's say you are trying to fly a helicopter. In this case you will not be able to have a STD cause helicopter dynamics are quite noisy. \n",
    "- One standard thing to do is to estimate the state transition probability from data.  \n",
    "- # $ P_{SA}(S') = \\frac{times\\ took\\ action\\ a\\ action \\ a\\ in\\ state\\ s\\ ,\\ got\\ state\\ s'\\ }{ times\\ took\\ action\\ a\\ in\\ state\\ s}$  \n",
    "- or $ \\frac{1}{|S|} if \\frac{0}{0}$ , like a uniform distribution over all states if you have never tried action a in state s.   \n",
    "- Putting it together: \n",
    "    - Repeat: \n",
    "        - Take actions using some policy $\\pi$ to get experience in $MDP$. \n",
    "        - Update estimates of your state transition probability based on the experience you gained. \n",
    "        - Solve Bellman's equation using Value Iteration to get an estimate for $V^*$. \n",
    "        - Update policy  $\\pi(s) := \\mathbf{argmax_A}  \\sum_{S'}P_{SA}(S')V^*(S')$   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
