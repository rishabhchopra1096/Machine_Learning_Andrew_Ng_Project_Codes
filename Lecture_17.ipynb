{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revising Lecture 16:   \n",
    "- $MDP: (S, A, P_{SA}, \\gamma, R)$\n",
    "- 11 states\n",
    "- A: {N, S, E, W} \n",
    "- $P_{SA}$: The state transition probability record the probability of you transitioning to every state, when you take any action in a state. \n",
    "- Eg: N: {0.8N, 0.1W, O.1E} \n",
    "- R = { $\\pm 1$ at the absorbing state}\n",
    "- R = -0.02 elsewhere. \n",
    "- $\\gamma = 0.99$ \n",
    "- Our goal was to find a control policy $\\pi: S -> A$ (which is function that maps a state to the action)  which maximises our total discounted reward.  \n",
    "- Total sum of discounted rewards: \n",
    "## $V^{\\pi}(s) = E[R(s_0) + \\gamma R(s_1) + \\gamma^2R(s_2) + .... | \\pi, s_1 = s]$  \n",
    "- The goal is to find the optimal policy which maximises the above rewards. \n",
    "- To do that: \n",
    "    - Find $V^*(s) = max_{\\pi}V^{\\pi}(s)$ \n",
    "        - The maximum reward that any policy can obtain. \n",
    "    - One you find $V^*(s)$ , you can compute the optimal policy using this formula: \n",
    "## ${\\pi}^*(S) = \\mathbf{argmax_A}  \\sum_{S'}P_{SA}(S')V^*(S')$   \n",
    "   - __Tells you what is the best action to take in each state by taking the argmax.__  \n",
    "- Therefore, the optimal value function is equal to: \n",
    "## $V^*(S) = R(S) + \\mathbf{max_A} \\gamma\\sum_{S'}P_{SA}(S')V^*(S')$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Everything we've done so far has been for discrete states or finite state $MDPs$. \n",
    "- For example, in our previous example, we had 11 seperate, finite states.\n",
    "- So, the value function $v(s)$ could be then represented using an array of 11 numbers. \n",
    "- Sidenote: \n",
    "    - Velocity: is the arate of change of the position of an object within a certain time range. \n",
    "        - For a car, it is the rate of change miles, with respect to hours \n",
    "    - There are two different types of velocity. \n",
    "        - Linear velocity is the rate of change of the position of an object that is traveling along a straight path. \n",
    "            - Any object that moves has a linear velocity.\n",
    "            - For instance, if people go for walk, drive, run, thereis linear velocity. \n",
    "            - At times, an object may be travelling with a constant speedon a straight path. This is the case of constant linear velocity. \n",
    "        - Angular Velocity only applies to objects that are moving along a circular path. \n",
    "            - A race car on a circular track. \n",
    "            - A roulette ball on a wheel. \n",
    "            - A Giant Wheel. \n",
    "        - Angular Velocity is the rate of change of the angular displacement with respect to time. \n",
    "            - When an object is tavelling a long a circular path, the central angle corresponding to the objects's position on the circle is changing. \n",
    "            - The angular velocity is the rate of change of the central angle with respect to time. \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- __Example__:\n",
    "    - A car has its position given by $x,y,\\theta$ and if you model the velocity as well, that is given by $x^., y^., \\theta^.$ \n",
    "    - Helicopeter: \n",
    "        - position: $ x,y,z, \\phi, \\theta, \\psi$ \n",
    "        - linear velocity: $x^., y^., z^.$ \n",
    "        - angular velocity: $\\phi^., \\theta^., \\psi^. $  \n",
    "    - Inverted Pendalum: \n",
    "        - $x$ : position of the cart\n",
    "        - $\\theta$: orientation of the pole \n",
    "        - $x^{\\mathbf{.}}$: Linear Velocity\n",
    "        - $\\theta^.$: Angular Velocity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization: \n",
    "__ How can you apply an alogorithmlike value iteration or policy iteration to solve the MDP for the car, helicopter or inverted pendalum? __   \n",
    "- Discretization is the most obvious way to turn a continuous state problem to a discrete state problem. \n",
    "- So for example, if you have 2 dimensional continuous state space. Then, you can discretize it into a number of discrete cells. Each cell corresponds to a discrete state with a number of _continuous value pairs_.  \n",
    "- $\\bar{S}$ refers to discrete states. \n",
    "- Then, you can use policy iteration or value iteration to solve for $V^*(\\bar{S}$) and $\\pi^*(\\bar{S})$ \n",
    "- So basically, if you end up in a state with $xyz$ orientation and $abc$ position. You can see where this pair is in the discrete table. See which discrete cell/state it belongs to, and then apply the policy thereon, from that state.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Why will this not work very well? __   \n",
    "- _Think of regression_  \n",
    "    - Discretization is basically saying that it will divide the x axis into buckets, and the y axis into buckets, and it will predict a constant value for a x bucket. Something like a decision tree regressor. \n",
    "    - We would not use such a model for regression. So similarly, if the x axis was $s$ for state and y axis was $v(s)$ for value funciton, we see that this does not make much sense. \n",
    "- _Curse of dimensionality _\n",
    "    - If your state space is in $S = R^n$. And if you discretize each state space into $K$ buckets. Then, you get $K^n$ discrete states. \n",
    "    - So basically, the number of discrete states you end up with, grows exponentially as you keep adding states. \n",
    "    - So for the helicopter, we had $12$ state space. For example, if we divided the each state space into $100$ buckets, we would have $100^{12}$ discrete states. \n",
    "- So, discretization scaled poorly to high dimensional state spaces. \n",
    "- As you increase you state space, you might manually fiddle with the choice of $K$ for each state. You can have non-unfirom discretization in which you use more buckets for states towards which the value function is more sensitive. \n",
    "- In other words, use more buckets for states that change the value function's value by alot just by changing its value  by a little. \n",
    "- Using this method, you can push discretization upto 6 dimensional problems. Beyond that, you cannot use discretization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set Up: \n",
    "- We are going to focus on continuous states. \n",
    "- We are not going to think about continuous actions. We are going to assume discrete set of actions. This is because its always easier to discretize the continuous set of actions as compared to continuous set of states. \n",
    "- There are always more states than actions. \n",
    "- For the Car, you have 6 states and 2D actions. \n",
    "- For the inverted pendalum, you have 4 states and you have 1D action (whether you move your cart tot he lef tor right). \n",
    "- So, for the rest of the lecture, we are going to assume that we have continuous states but we have already _discretized_ our actions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function approximation: \n",
    "- We now describe an alternative method for finding policies in continuous state MDPs, in which we approximate the optimal value function ($V^*$) directly, without resorting to discretization. \n",
    "- This approach, called value function approximation, has been successfully applied to many RL problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume we have a model/simulator of MDP.This is just an assumption on how the state transition probabilities are represented. \n",
    "- Assume we have a black box (piece of code) in which i input a state and an action, and it will output a new state according to the state transition probabilities: $P_{SA}$.  \n",
    "\n",
    "- Eg: Physics simulator  \n",
    "- In the inverted pendulum problem, if your action is represented $a$, which is the magnitude of the force you exert on the car  (to the left or right, say) and your state is: \n",
    "    - $ S = ${$ X, \\dot{X}, \\theta, \\dot{\\theta}$}\n",
    "    - Then, you can work out all the equation you need to detemine the next state of the model, give your current action.   \n",
    "    \n",
    "- __Learning a Model: __   \n",
    "    - Imagine you physically own an inverted pendulum robot. \n",
    "    - Initialise your robot to some state $s_0$ \n",
    "    - Execute some policy. A random policy, or you can even try executing it yourself by using a joystick. \n",
    "    - You keep taking some action, which will make you transition to a new state $s_1$ and you keep doing this for $t$ time steps. \n",
    "    - $S_0^1 >a_0^1> S_1^1 >a_1^1> S_2^1 >a_2^1> S_3^1 ....>a_{t-1}^1> S_t^1$ \n",
    "    - And then you do this again. \n",
    "    - $S_0^2 >a_0^2> S_1^2 >a_1^2> S_2^2 >a_2^2> S_3^2 ....>a_{t-1}^2> S_t^2$ \n",
    "    - And then you keep doing this for $m$ trials. \n",
    "    - $S_0^m >a_0^m> S_1^m >a_1^m> S_2^m >a_2^m> S_3^m ....>a_{t-1}^m> S_t^m$ \n",
    "    - Run a supervised learning algorithm to estimate $S_{t+1}$ as a funciton of $S_t$ , $a_t$ like linear regression. \n",
    "        - $S_{t+1}$ is a 4 dimensional vector. \n",
    "        \n",
    "    - For eg: \n",
    "        - $S_{t+1} = A_{S_t} + B_{a_t}, A \\in R^{4x4}, B\\in R^{4}$\n",
    "        - ${S_t}$ represents previous state.   \n",
    "        DOUBT: Does each row represent the state you will end up in if you take action $a$ ?  \n",
    "        - Why is $A$ a 4x4 matrix? \n",
    "            - Cause 4x4 * 4*1 = 4*1 \n",
    "            - and then $Ba_t$ is also a 4*1 column vector. \n",
    "        - ${a_t}$ represents the action you just took. \n",
    "- Turns out, locally weighted linear regression turns out to be a good option here. \n",
    "    - After training, the model is used for predictions and the data are generally discarded. ... Locally weighted regression (LWR) is a memory-based method that performs a regression around a point of interest using only training data that are `local` to that point.  \n",
    "- Model = $S_{t+1} = A_{s_t} + B_{a_t}$ (deterministic) \n",
    "- or \n",
    "- $S_{t+1} = A_{s_t} + B_{a_t} + \\epsilon a$ (Stochastic) \n",
    "- where $\\epsilon ~ N(O,\\sum)$ \n",
    "- $S_t , a - > Model -> S_{t+1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a model/simulator for your MDP, how to come up with an algorithm to approximate the optimal value function. \n",
    "- When we talked about Linear Regression, we were given some inputs $X$, we choose some  features of $x$  and predict $y$ as a linear function of features of $x$. \n",
    "- And so, we are going to do a similar thing to approximat ethe optimal value function. \n",
    "- In particular, we choose some features $\\phi (s)$ of state $s$. \n",
    "    - You could choose $\\phi(s) = s$ \n",
    "    - For the inverted pendalum problem, you might choose: \n",
    "        -  $\\phi(s) = [ 1, X, \\dot(X)^2, \\dot(X), \\theta.X, \\theta^2] $ \n",
    "        - Approximate $ V(S) = \\theta^T\\phi(s)$ \n",
    "        - Note that $\\theta$ stands for 2 things here: \n",
    "            - the orientation of the inverted pendalum. \n",
    "            - the parameter vector. \n",
    "- So just like we had in linear regression, our goal is to come up with a linear combination of the features that gives me a good approximation to the value funciton. \n",
    "- Note that, just like in linear regression, we had: \n",
    "    - $h_{\\theta}(x) = \\theta^T.X = \\theta^T\\phi(x)$  \n",
    "- DOUBT: What policy are we following when we calculate V(S)?\n",
    "    - There is no policy man. V(S) is being calculated as a linear combination of the sub-state space $\\phi(s)$ and the parameter vector $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration:  \n",
    "- For discrete states: \n",
    "    - Initialise $V(S) = 0$ $\\forall S$: Create an array of $n$ elements where $n$ is the the number of states. \n",
    "    - For every S, update:  \n",
    "        - $V(S) := R(S) + max_A \\gamma \\sum_{S'} P_{SA}(S')V(S')$  \n",
    "        - $V(S) := R(S) + max_A \\gamma E_{S'\\in P_{SA}} [V(S')]$ \n",
    "    - This will make $V(S) -> V^*(S)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Fitted Value Iteration:    \n",
    "We cna now describe the fitted value iteration algorithm for approximating the value function of a continuous state MDP. \n",
    "- In the sequel, we will assume that the problem has a continuous state space $S = R^n$, but the that action space $A$ is small and discrete.  \n",
    "\n",
    "- The main idea of fitted value iteration is that we are going to approximately carry out the value iteration step:\n",
    "    - $V(S) := R(S) + max_A \\gamma E_{S'\\in P_{SA}} [V(S')]$ \n",
    "- over a fintie sample of states $s^1, ...., s^m$. \n",
    "- Specifically, we will use a supervised learning algorithm-linear regression in our description below to approximate the value function as a linear or non-linear function of the states: \n",
    "    - $ V(S) = \\theta^T \\phi(S)$ \n",
    "- For each state $S$ in our finite sample of $m$ states, fitted value iteration will first compute a quantity $y^i$ , which will be our approximation to the optimal value function for that state: $ R(s) + \\gamma max_a  E_{S' \\in P_{SA}} [V(S')]$. \n",
    "- Then, it will apply a supervised learning algorithm to try to get $V(S)$ close to $y^i$. \n",
    "\n",
    "- For continuous states: \n",
    "    - Choose a set of m states from the state space $(S = R^n$)  randomly $ = \\phi(s)$. \n",
    "    - Intiialise the parameter vector $\\theta := 0$ \n",
    "\n",
    "- Repeat __{'__  \n",
    "    - For $i = 1, ..., m$ __{''__ `# for all states` \n",
    "        - For each action __{'''__ $a \\in A$ `#for all actions`   \n",
    "            - Sample $s'_1,....,s'_k$ ~ $P_{s^ia}$ (using a model of the MDP)   \n",
    "            `# sample all the possible outcomes after taking action a`  \n",
    "            \n",
    "            - Set $ q(a) = \\frac{1}{k} \\sum_{j=1}^{k} R(s^i) + \\gamma V(s'_j)$  \n",
    "                - `# Set q(a) equal to Average over all states you end up in: {the immediate reward plus  gamma times the value function of the state you end up in.}`   \n",
    "                - ` // Hence, q(a) is an estimate of` $R(s) + \\gamma E_{S' \\in P_{sa}} [V(s')]$     \n",
    "                \n",
    "                     __}'''__  \n",
    "                     \n",
    "        - Set $ y^i = max_a q(a)$  \n",
    "            - `# Hence, `$y^i$ `is an estimate of` $R(s^i) + \\gamma \\mathbf{max_a} E_{s' \\in P_{s^ia}}[V(s')]$     \n",
    "                __}''__   \n",
    "            - `// In the original value iteration algorithm (over discrete state) , we updates the value function according to` $V(s^i) := y^i$ . \n",
    "            - `// In this algorithm, we want` $V(s^i) \\approx y^i$ `, which we'll achieve using supervised learning (linear regression).` \n",
    "    -  Set $\\theta: = arg min_\\theta \\frac{1}{2}\\sum_{i = 1}^m(\\theta_T \\phi(s^i) - y^i)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The heart of the algorithm is: \n",
    "    - In the original value iteration algorithm, we would take the value for each state $V(s^i)$ , and we would overwrite it with $R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$.\n",
    "- In the continuous state case, we have an infinite set of states so you cannot discretely set the value of each state to $R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$.\n",
    "- So what we'll do is choose the parameters $\\theta$ so that $V(s^i)$ is as close as possible to ($R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$ or $y^i$ or $max_a q(a)$). \n",
    "- So what we do here is we construct estimates of the optimal value function for each state ($R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$ or $y^i$ or $max_a q(a)$)\n",
    "- Then, we are going to choose the parameters $\\theta$  of our function approximator so that $V(s^i)$ is as close as possible to the __optimal value function estimate __. ($R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$ or $y^i$ or $max_a q(a)$)\n",
    "\n",
    "\n",
    "- Concretely, for every state, we want to compute a __optimal value function estimate__ ($R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$ or $y^i$ or $max_a q(a)$) \n",
    "- The expectation here: $E_{s' \\in P_{s^ia}}[V(s')]$ is the expectation over the states you might end up in if you take action $a$ in state $s$.  The problem with this expectation is that there are again, a continuous set of states you might end up in. So what you can do is, set $k$ , the number of states over which you want to calculate the expectation. \n",
    "- To calculate the __optimal value function estimate__(($R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$ or $y^i$ or $max_a q(a)$), you first need to see the expected value by taking each action $q(a) = \\frac{1}{k} \\sum_{j=1}^{k} R(s^i) + \\gamma V(s'_j)$  , and then take the max over it. \n",
    "- The max with be our $y^i = max_a q(a) = R(s^i) + \\mathbf{max_a}E_{s' \\in P_{s^ia}}[V(s')]$  \n",
    "- Then, we will fit our parameters $\\theta$ to our sub-state space for each state. \n",
    "- This works well with 6-20 dimensional state spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above, we had writted our fitted value iteration using linear regression as the algorithm to try to make $V(s^i)$ close to $y^i$. \n",
    "- That step of the algorith is completely analogous to a standard supervise learning (regression) problem in which we have a training set $(x^1, y^1), (x^2, y^2), ... (x^m, y^m)$, and want to learn a function mapping from $x$ to $y$; the only difference is that here $s$ play the role of $x$. \n",
    "- Even though our description above used linear regression, clearly other regression algorithms (such as localy weight linear regression) can also be used. \n",
    "- Unlike value iteration over a discrete set of states, fitted value iteration cannot be proved to always converge. \n",
    "- However, in practice, it often does converge (or approximately converge), and works well for many problems. \n",
    "- Note also that if we are using deterministic simulator/model of the MDP, then fitted value iteration can be simpliefied by setting $k=1$. in the algorithm. \n",
    "- This is because the expection in the equation: $R(s) + \\gamma E_{S' \\in P_{sa}} [V(s')]$    becomes an expectation over a deterministic distributio, and so a single example is sufficient to exactly compute that expectation. \n",
    "- Otherwise, in the algorithm above, we had to draw $k$ samples, and average to try to approximate that expectation. \n",
    "- Finally, fitted value iteration outputs $V$, which is an approximation to $V^*$. \n",
    "- This implicitly defined our policy. Specifically, when our system is in some state $s$, and we need to chose an action, we would like to choose action: \n",
    "    - $arg max_a E_{s' \\in P_{sa}}V(s')$ \n",
    "- The process for computing/approximating this is similar to the inner-loop of fitted value iteration,, where for each aciton we sample states to aprroximate the expectation. \n",
    "- In practice, there are often other ways to approximate this step as well. \n",
    "- For example, one very common case is if the simulator is of the for $s_{t+1} = f(s_t , a_t) + \\epsilon_t$ where f is some deterministic function of the states (such as $f(s_t, a_t) = As_t + Ba_t)$, and $\\epsilon$   is zaro-mean gaussian nosie. \n",
    "- In this case, we can pick the action given by:\n",
    "    - $ argmax_a V(f(s,a))$ \n",
    "    - The action that returns the max value of the next state.\n",
    "- HEre, we are just setting $\\epsilon_t = 0$ (ignoring noise in the simulator), and setting $k=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
