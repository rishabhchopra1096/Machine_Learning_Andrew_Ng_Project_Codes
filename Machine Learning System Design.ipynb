{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Spam Classifier: \n",
    "### Prioritizing what to work on: \n",
    "\n",
    "- Find the most frequently used words in your dataset (10,000 - 50,000). These words will become your features for each email. \n",
    "- Given a data set of emails, we could construct a vector for each email. \n",
    "- Each entry in this vector represents a word.  \n",
    "- If a word is to be found in the email, we would assign its resepective entry a 1, else if is not found, that entry would be 0.\n",
    "- Therefor, one element of the input matrix $X$ will be $x_j = 1 $ if $word_j$ appears in email, $0$ otherwise. \n",
    "- Once we have all our x vectors ready, we train our alogorithm and finally, we could use it classify if an email is spam or not. \n",
    "\n",
    "- __ What should you prioritize on to improve the accuracy of this classifier? __\n",
    "    - Collect lots more data\n",
    "    - Develop sophisticated features (using email head data in spam emails)\n",
    "    - Develop algorithms to process your input in different ways (recognizing misspellings in spam)\n",
    "\n",
    "- IT IS DIFFICULT TO TELL WHICH OPTIONS WILL BE MOST HELPFUL. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis: \n",
    "\n",
    "- The recommended approach to solving machine learning problems is to :\n",
    "    - Start with a simple algorithm, implment it quickly, and test it early on your cross validation data.\n",
    "    - Plot learning curved to decide if more data, mor features, etc are likely to help or not. \n",
    "    - Error Analysis: Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made. \n",
    "\n",
    "- For example,\n",
    "    - assume that we have 500 emails and our algo misclassifies 100 of them. \n",
    "    - We could manually analyze the 100 emails and categorize them based on what __category__ of emails they are. Not label, _category_ (like pharma, steal pass, other types). \n",
    "        - So, for instance, if most of our misclassified email are those which try to steal passwords, then we could find some features(words) that are particular to those emails and add them to the model. \n",
    "        - Other types of emails may be Pharmacy(Drugs), Relica (fake) , Other. \n",
    "    - We could then try to come up with __new cues and features__ that would help us classify these 100 emails correctly. \n",
    "        - For example, a lot misclassified emails contain:\n",
    "            - Deliberate misspellings\n",
    "            - Unusual email routing\n",
    "            - Unusual putuaction\n",
    "    - Splitting the misclassified emails into categories on the basis of type of email or on the basis of certain features/cues they have help you determine on what would you like to concentrate.\n",
    "        - For example: If the number of misclassified emails due to deliberate misspelling is only 2% of the whole misclassified emails, you will not put too much work oon developing an algo that detects deliberate misspellings. On the other hand, if unusual putuation account for 30% of the misclassified emails, then you would definitely work on an algo that catched unsual puntuation. \n",
    "        - Similarly, if the number of misclassified (drug-selling) emails account for only 4% of the total misclassified emails, you would not search for features that will help in detecting a pharmacy mail. On the other hand if most of our misclassified email are those which try to steal passwords, then we could find some features(words) that are particular to those emails and add them to the model. \n",
    "          \n",
    "    \n",
    "__The importance of a numerical evaluation: __\n",
    "- It is very important to get the error results as a single, numerical value (like accuracy, r2 score, MSE). Otherwise, it is difficult to asses your algorithms's performance. \n",
    "- For eg: If we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Metrics for Skewed Classes: \n",
    "\n",
    "__1. Demonstrate why classification accuracy can be a poort measure of performance__\n",
    "- Training a logistic model $h_{\\theta}x$ - (y = 1 if cancer , y= 0 if otherwise)\n",
    "- You find that you got 1% error on test set. That is, you were right about 99% of the time. Wow! That's impressive. \n",
    "- Suppose now, we find out that 0.50% of patients have cancer. That means, in proportions: 0.005 of our patients have cancer. \n",
    "- Now, the 1% error does not seem so great. \n",
    "- We could have this non-learning function: \n",
    "\n",
    "```python\n",
    "\n",
    "def predict(X): \n",
    "    y = [0]*len(X) # ignores X, always predicts No Cancer\n",
    "    return y\n",
    "    \n",
    "predict(X)\n",
    "#>> [0.  0.  0.  0. 0. ......]\n",
    "```\n",
    "- This will get an accuracy level of 99.5%. Does not seem nice right? \n",
    "- This is an example of skewed classes. This is wherein, the number of examples belonging to a certain class far exceed the number of examples belonging to other classes. \n",
    "\n",
    "__2. Use a $2 \\times 2$ table comparing the ground truth to predictions to describe the performance of a model. __\n",
    "\n",
    "__Precision/Recall__\n",
    "y = 1, which is rate class that we want to catch/detect.  \n",
    "\n",
    "- 1 = Cancer, 0 = No Cancer   \n",
    "- Actual Class 1, Predicted Class 1: True Positive\n",
    "- Actual Class 0, Predicted Class 0: True Negative\n",
    "- Actual Class 0, Predicted Class 1: False Positive\n",
    "- Actual Class 1, Predicted Class 0: False Negative\n",
    "\n",
    "__3. Computing Precision__  \n",
    "\n",
    "- __Precision__: Out of all the points classified as postive, how many were actually positive? \n",
    "- Out of all the patients predicted to have cancer, how many did actually have cancer? \n",
    "    - Accuracy of positive predictions\n",
    "    - # $\\frac{TP}{TP + FP}$\n",
    "    - Number of True positives divided by the total number of points predicted to be positive (including the points that were correctly classified as positive and the points that were incorrectly classified as positives) \n",
    "    - High(90%) precision means that the group of patients we went to and said \"Sorry, you have cancer. We were right about 90% of the time. The other time, it was a false alarm ( false positive) and the patient had to go through the medication for nothing. \n",
    "    - Low precision(30%) means that the group of patients that we went to and said \"Sorry, you have cancer. We were right only about 30% of the time. The other 70% time, it was a false alarm ( false positive) and the patient had to go through the medication for nothing. \n",
    "    - Using the old example, where we predict y = 0 for every example. We would get a precision of 0, as no points are classifed as postive. \n",
    "    - On the other hand, even if we predicted all the examples as postives y = 1, then our precision will be: \n",
    "        - # $\\frac{0.005(TP)} {0.005(TP) + 0.995(FP)} = 0.005$\n",
    "    \n",
    "__4. Computing Recall__ \n",
    "- __Recall__: Out all the points that are actually positive, how many do we correctly classify as positive? \n",
    "- Out of all tha patients who had cancer, how many did we correctly classify as having cancer? \n",
    "    - How many of the positive labels do we catch correctly? \n",
    "    - # $\\frac{TP}{TP + FN}$ \n",
    "    - Number of points correctly classified as positives divided by the total number of positives points in the dataset (including the points correctly classified as postivies and the points incorrectly classified as negatives ) \n",
    "    - High(90%) Recall is good. This means that the out of all the people who did have cancer, we caught 90% of the people. The other 10% did have cancer, but we classified them as not having cancer, leaving them to die :). \n",
    "    - Low Recall is bad. This means that out of all the people who had cancer, we only caught 30% of the cases, leaving the other 70% cancer strucken people to die without treatment. \n",
    "    - Using the old example, where we predict y = 0 for every example. We would get a recall of 0, as no points are classifed as postive.\n",
    "    - On the other hand, even if we predicted all the examples as postives y = 1, then our precision will be: \n",
    "        - # $\\frac{0.005(TP)} {0.005(TP) + 0(FN)} = 1$\n",
    "        \n",
    "- __More generally, if we have a very simple prediction function. It is not possible for the prediction function to quote on quote \"cheat\"  to get a high precision and a high recall.__\n",
    "- An algorithm with both high precision and high recall tell us that the algorithm catches the majority of the postive class points with high accuracy. In other words, it _classifies_ a high majority of the positive points _as positive_ (recall) , _with a good accuracy_(precison).\n",
    "- Final Note: By convention, If we try to detect some rare class (cancer, credit card fraud, etc.) we set y = 1 for the more rate class that we want to capture/detect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoff Precision and Recall\n",
    "\n",
    "\n",
    "__1. Showing that different threshold for a classifier can change precision and recall__\n",
    "\n",
    "Now that we have talked about precision and recall as evaluation metrics for classification problems involving skewed classes, for many applications we would want to control the tradeoff between precision and recall.  \n",
    "\n",
    "Suppose we have a logistic regression classifiers: $ 0 <= h_{\\theta}x <= 1 $ that will predict $1$ (Cancer) if $ h_{\\theta}x >= 0.5 $ and will predict $0$ (No Cancer) if $h_{\\theta}x < 0.5 $. \n",
    "\n",
    "__High Precision Model: __\n",
    "- Now, suppose we want to predict that a patient has cancer very confidently/conservatively. We want to be very sure that a person has cancer before we tell a patient that he/she does have cancer. Otherwise, the patient will have to suffer the trauma, and moreover eat medicines he she doesn't need to. \n",
    "    - In other words, we want the accuracy of our positive predictions to be high. When we say a patient has cancer, he/she better have cancer. \n",
    "    - One way to do this, is to modify the algorithm. In other words, increase the threshold from 0.5 to 0.7. \n",
    "    -  $ 0 <= h_{\\theta}x <= 1 $ that will predict $1$ (Cancer) if $ h_{\\theta}x >= 0.7 $ and will predict $0$ (No Cancer) if $h_{\\theta}x < 0.7 $. \n",
    "    - This means that we will predict a 1, only when we get a higher activation/decision score. \n",
    "    - This will make the accuracy of the postive prediction higher. But in contrast, we will be able to catch less of the cancer patients, decreasing the recall. \n",
    "    - Increasing the threshold more and more, will increase the precision as we will be predicting `y=1`, or cancer, only when we get a high decision score. \n",
    "\n",
    "__High Recall Model: __\n",
    "- Now, suppose we want to catch at many cancer patients as possible. Catch meaning classify a patient positive, when he/she is truely positive. Otherwise, some cancer patient may get classified as healthy, and die without any treatment. \n",
    "    - In this case, we can decrease the threshold from 0.5 to 0.3. \n",
    "    -  $ 0 <= h_{\\theta}x <= 1 $ that will predict $1$ (Cancer) if $ h_{\\theta}x >= 0.3 $ and will predict $0$ (No Cancer) if $h_{\\theta}x < 0.3 $. \n",
    "    - This means that we will predict a 1, even if we do not get a really high activation/decision score. \n",
    "    - This will make us catch alot of the cancer patients. Catch meaning, classify a positive as positive, or classifying a person who has cancer as having cancer. \n",
    "    - But in contrast, our precision/ accuracy of the positve prediction will go down as we will be classifying a lot of healthy patients as having cancers (increasing False Positives). \n",
    "    - Decreasing the theshold more and more will increase the recall as we will be classifying a patient as positive(having cancer) even when we are a little doubtful. \n",
    "\n",
    "__Is there a way to choose the threshold automatically?__  \n",
    "\n",
    "__ 2. Explaing why the average of precision and recall does not provide a good performance__:  \n",
    "- For example: When we predict all the patient have cancer, we get a precision of:\n",
    "    - # $\\frac{0.005(TP)} {0.005(TP) + 0.995(FP)} = 0.005$\n",
    "    - and a recall of:  \n",
    "    - # $\\frac{0.005(TP)} {0.005(TP) + 0(FN)} = 1$\n",
    "    - Averaging to 0.5025 or 50.25%, which is not a good measure for the model. \n",
    "    \n",
    "__3. Computing the F - score on a dataset __ \n",
    "\n",
    "# $F_1 Score = 2 \\frac{PR}{P + R}$\n",
    "- If either Precision and Recall is low, the F_1 score will be low as are taking the product of P and R. \n",
    "- For F_1 score to be large, both precision and recall need to be large. \n",
    "- If either P or R is 0, F_1 score is 0. \n",
    "- If both P and R are 1, F_1 score is 1. \n",
    "\n",
    "__How to use this?__\n",
    "- Measure Precision and Recall on the cross validation set for different thresholds and choose the value of the threshold which maximises F_1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
